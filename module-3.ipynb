{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adityeah18/tensorflow/blob/main/module-3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBVX_vVp7RgU"
      },
      "source": [
        "#Module-3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzRYlWIV6CjX"
      },
      "source": [
        "## ðŸ“š Convolutional Neural Networks (CNN) â€“ Complete Explanation\n",
        "\n",
        "### âœ… CNN vs DNN â€“ Core Differences\n",
        "\n",
        "| Feature         | DNN (Deep Neural Network)                          | CNN (Convolutional Neural Network)                     |\n",
        "|----------------|----------------------------------------------------|--------------------------------------------------------|\n",
        "| Input Type     | Flat structured data                               | 3D structured data (Image: H Ã— W Ã— Channels)           |\n",
        "| Layer Type     | Dense (Fully Connected)                            | Convolutional + Pooling + Dense                       |\n",
        "| Pattern Type   | Global pattern recognition                         | Local pattern recognition                             |\n",
        "| Image Handling | Poor with spatial changes, sensitive to shifts     | Good with translations, detects features anywhere     |\n",
        "| Efficiency     | High computation on images                         | More efficient due to local connections and weight sharing |\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§  CNN Architecture Concepts\n",
        "\n",
        "### 1. ðŸ“ Input Dimensions\n",
        "\n",
        "- Shape: **Height Ã— Width Ã— Channels**\n",
        "- Channels:\n",
        "  - RGB image â†’ 3 channels\n",
        "  - Grayscale â†’ 1 channel\n",
        "- Pixel range: **0 to 255** (usually normalized to 0â€“1)\n",
        "\n",
        "---\n",
        "\n",
        "### 2. ðŸ§° Filters (Kernels)\n",
        "\n",
        "- Learnable matrices: e.g., `3x3`, `5x5`\n",
        "- Slides over image to extract features like:\n",
        "  - Edges\n",
        "  - Textures\n",
        "  - Patterns\n",
        "- Each filter outputs a **Feature Map**\n",
        "\n",
        "---\n",
        "\n",
        "### 3. âš™ï¸ Convolution Operation\n",
        "\n",
        "- Filter **slides** across input\n",
        "- At each step:\n",
        "  - Performs **dot product** between filter and image patch\n",
        "  - Result is stored in **feature map**\n",
        "- Deeper layers = more abstract features\n",
        "\n",
        "---\n",
        "\n",
        "### 4. ðŸ§â€â™‚ï¸ Stride\n",
        "\n",
        "- How many pixels the filter moves at a time:\n",
        "  - `stride = 1`: moves one pixel â†’ high-resolution output\n",
        "  - `stride = 2`: skips pixels â†’ faster but lower-res\n",
        "\n",
        "---\n",
        "\n",
        "### 5. ðŸ§± Padding\n",
        "\n",
        "- Adds borders to input so the filter can cover the edges\n",
        "- Types:\n",
        "  - `valid` â†’ no padding (output shrinks)\n",
        "  - `same` â†’ zero-padding to keep size same as input\n",
        "\n",
        "---\n",
        "\n",
        "### 6. ðŸ’§ Pooling (Downsampling)\n",
        "\n",
        "- Reduces dimensionality\n",
        "- Helps generalization and speed\n",
        "- Common types:\n",
        "  - **Max Pooling**: keeps max value in patch\n",
        "  - **Average Pooling**: takes average\n",
        "- Example: `2x2 Max Pooling` halves H and W\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”„ Typical CNN Layer Flow\n",
        "\n",
        "```text\n",
        "Input Image (32x32x3)\n",
        "â†“\n",
        "Conv Layer (with 3x3 filters)\n",
        "â†“\n",
        "ReLU Activation\n",
        "â†“\n",
        "Pooling Layer (2x2)\n",
        "â†“\n",
        "Flatten\n",
        "â†“\n",
        "Dense Layer\n",
        "â†“\n",
        "Softmax Output (for classification)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL-XLXfGITba"
      },
      "source": [
        "## CNN Representation\n",
        "# ðŸ§  CNN Architecture Visual Diagram\n",
        "\n",
        "Below is a basic representation of how a Convolutional Neural Network processes an image input:\n",
        "\n",
        "```text\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  Input Image       â”‚   â† [32 x 32 x 3] (H x W x Channels)\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "           â”‚\n",
        "           â–¼\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ Convolution Layer  â”‚   â† Apply multiple filters (e.g., 3x3)\n",
        "â”‚ (Conv2D)           â”‚   â†’ Feature maps: edges, textures\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "           â”‚\n",
        "           â–¼\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ Activation (ReLU)  â”‚   â† ReLU introduces non-linearity\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "           â”‚\n",
        "           â–¼\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ Pooling Layer      â”‚   â† MaxPooling2D (e.g., 2x2)\n",
        "â”‚ (Downsampling)     â”‚   â†’ Reduces spatial size\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "           â”‚\n",
        "           â–¼\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ More Conv + Pool   â”‚   â† Stack deeper layers to learn\n",
        "â”‚ (Optional)         â”‚      complex patterns\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "           â”‚\n",
        "           â–¼\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ Flatten Layer       â”‚  â† Converts 2D feature maps into 1D\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "           â”‚\n",
        "           â–¼\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ Fully Connected     â”‚  â† Dense layer\n",
        "â”‚ (Dense Layer)       â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "           â”‚\n",
        "           â–¼\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ Output Layer        â”‚  â† e.g., 10 classes â†’ Dense(10, softmax)\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_yOOgfaBDxT"
      },
      "source": [
        "Essential Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5SaSJI1AxqL"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.__version__\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lec6V4AkALNM"
      },
      "source": [
        "##Dataset\n",
        "CIFAR dataset = 60000 samples , 10 Labels, 6000 per labels\n",
        "labels= Airplane,Automobile,Bird,Car,Deer,Dog,Frog,Horse,Ship,Truck"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJ8KmJaOBi3W"
      },
      "source": [
        "###Spliting /Loading and other preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHW8Y7Fr5978"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import datasets\n",
        "(train_image,train_label),(test_image,test_label)= datasets.cifar10.load_data() ## why there are so many watyys to load datset?\n",
        "#Now the images of training and testing are ranging from 0-255 as being in color\n",
        "#We have to Normalize it\n",
        "#Normalize\n",
        "train_image,test_image=train_image/255.0,test_image/255.0\n",
        "\n",
        "class_name=['airplane','automobile','bird','car','deer',\n",
        "            'dog','frog','horse','ship','truck'] #10 labels which gonna be used in ploting the data\n",
        "#Let's see the image example\n",
        "IMG_INDEX=7\n",
        "\n",
        "plt.imshow(train_image[IMG_INDEX])\n",
        "plt.xlabel(class_name[train_label[IMG_INDEX][0]])\n",
        "plt.show()\n",
        "#As result image 7 = somwhat a horse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WTfuNwaEZdJ"
      },
      "source": [
        "##CNN Architecture\n",
        "the architecture is simple:\n",
        "1st stack of Convolution process(Filter ) and then Pooling method(extracting the features from images) and these are Flattend and the fed to CNN to determine which class it fall s under"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_gz1AjuNPRS"
      },
      "source": [
        "###model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AYLwbFSEZLU"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers,models\n",
        "model=models.Sequential()\n",
        "#Convolution stacking\n",
        "model.add(layers.Conv2D(32,(3,3),activation='relu',input_shape=(32,32,3))) #Making the matrix of 32 and then selecting 3x3 of input image matrix of 32x32x3(3=RGB)\n",
        "#Now the Pooling ;here max pooling we took\n",
        "model.add(layers.MaxPooling2D(2,2)) #After the filter the image goes throug th feature extraction in 2x2 matrix here takes the max from the 2x2 matrix\n",
        "\n",
        "#Extra layers to add deptha nd good learning\n",
        "#But as we alreaady try to limit t he computational usage, doest this increase it again?\n",
        "model.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "model.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "\n",
        "model.summary() ## ask the gpt as it suppuse to show the depth increase bur special dimention reduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvdFMGuFNR4j"
      },
      "source": [
        "###Model Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bf2Xjaq8MGh9"
      },
      "outputs": [],
      "source": [
        "#Now flattening the data\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "#Adding the Dense layer to classify the images in the labels\n",
        "model.add(layers.Dense(64,activation='relu'))\n",
        "\n",
        "#Output\n",
        "model.add(layers.Dense(10,activation='softmax')) #as the ere is 10 Labels\n",
        "\n",
        "model.summary() ## again what its telling and how to read???"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYokFVKcNVbJ"
      },
      "source": [
        "##Compile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtHgxyggNhKm"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import losses\n",
        "\n",
        "model.compile(optimizer='adam',loss='SparseCategoricalCrossentropy',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbNR7K-KTB1u"
      },
      "source": [
        "##Fitting/Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1aEE-0dN-LJ"
      },
      "outputs": [],
      "source": [
        "model.fit(train_image,train_label,epochs=10,validation_data=(train_image,train_label))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkzfGNM0TG_Z"
      },
      "source": [
        "##Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rj6W7KpmPJ5y"
      },
      "outputs": [],
      "source": [
        "loss,accuracy=model.evaluate(test_image,test_label,verbose=2)\n",
        "print(f'the accuracy is:{accuracy:.04f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCu-HKs5TZ0S"
      },
      "source": [
        "---\n",
        "##Working with small datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Na4XQ55Ta-Y"
      },
      "source": [
        "###Data Augmentation\n",
        "It s a technique used to artificially expand the size of a training dataset by creating modified versions of images.\n",
        "\n",
        "These modifications can include:\n",
        "\n",
        "Rotation,Width/height shift,Zoom in/out, Horizontal/vertical flip,Brightness/contrast adjustment, Shear/stretch/skew\n",
        "\n",
        "This helps the model:\n",
        "\n",
        "Learn features from different perspectives. Become robust to variations, Generalize better to unseen data\n",
        "\n",
        "ðŸ§  In short: Instead of collecting 10,000 new images, you can teach your model to be smart by twisting and remixing your 1000 images in creative ways.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgX0wH3lTZlH"
      },
      "outputs": [],
      "source": [
        "#Importation for image augmentation\n",
        "#understand the working not the syntax\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "#Data generator objectes for parameters to activate when processing the image\n",
        "datagen=ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "#Randomply selecting an image to transform\n",
        "img_gen_test=train_image[20]\n",
        "#converting image into numpy array\n",
        "img=image.img_to_array(img_gen_test)\n",
        "#Rehaping\n",
        "img=img.reshape((1,)+img.shape)\n",
        "\n",
        "i=0\n",
        "\n",
        "for batch in datagen.flow(img,save_prefix='test',save_format='jpeg'):\n",
        "  plt.figure(i)\n",
        "  plot=plt.imshow(image.img_to_array(batch[0]))\n",
        "  i+=1\n",
        "  if i>4:\n",
        "    break\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx1N3EDh7-AB"
      },
      "source": [
        "###Pretrained dataset\n",
        "CNNs trained on massive datasets (like ImageNet, which has over 1 million images across 1000+ classes) can be reused for transfer learning. Instead of training a CNN from scratch, we can:\n",
        "\n",
        "Use the pretrained CNN as a feature extractor, and\n",
        "\n",
        "Attach our own custom classifier (like a DNN) at the end for a specific task.\n",
        "\n",
        "ðŸ“Œ Think of it like this: the CNN already knows how to detect edges, textures, shapes â€” so why re-invent the wheel?\n",
        "\n",
        "---\n",
        "\n",
        "####Fine Tuning\n",
        "Fine-tuning is the process of unfreezing some layers of the pretrained model and retraining them (usually the deeper ones) on your specific dataset.\n",
        "\n",
        "Why?\n",
        "\n",
        "Early layers in a CNN learn general features like edges, corners, textures. These are useful across all kinds of images.\n",
        "\n",
        "Later layers learn task-specific features (like cat faces, dog paws, etc.). You might want to tweak these to match your dataset (like medical scans, satellite images, etc.).\n",
        "\n",
        "So instead of retraining the entire model, we just fine-tune the final layers to improve performance without starting from zero.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jM58ewwC-ehL"
      },
      "source": [
        "###Using these pretrained models\n",
        "We will seperate the Cats with dogs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDNrma9F7rPk"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar #??\n",
        "#Spliting the datset into training, validation and testing\n",
        "(raw_train, raw_validation, raw_test), metadata = tfds.load(\n",
        "    'cats_vs_dogs',\n",
        "    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
        "    with_info=True,\n",
        "    as_supervised=True,\n",
        ")\n",
        "#I don't understand in the same why the loading and test_train image extraction is so different????\n",
        "#I mean why not this kinda approach (train_image,train_label),(test_image,test_label)= datasets.cifar10.load_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tp34NsanBbCc"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "get_label_name = metadata.features['label'].int2str  # creates a function object that we can use to get labels\n",
        "\n",
        "# display 2 images from the dataset\n",
        "for image, label in raw_train.take(5):\n",
        "  plt.figure()\n",
        "  plt.imshow(image)\n",
        "  plt.title(get_label_name(label))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EahojxbgCFsy"
      },
      "source": [
        "###Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jXf1MpcCFaJ"
      },
      "outputs": [],
      "source": [
        "## as seen from above the side of the images are different\n",
        "#We try to have a image size which can compress but not strech coz then it makes hard to detect which image is this\n",
        "#For this datset we well have image size=160\n",
        "#IMG_SIZE=160 #160x160\n",
        "import tensorflow as tf\n",
        "\n",
        "def format(image,label):\n",
        "  image=tf.cast(image,tf.float32)\n",
        "  image=(image/127.5)-1\n",
        "  image=tf.image.resize(image,(160,160))\n",
        "  return image,label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xRR75nXEKB_"
      },
      "outputs": [],
      "source": [
        "train = raw_train.map(format)\n",
        "validation = raw_validation.map(format)\n",
        "test = raw_test.map(format)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcvZWrE5EPWa"
      },
      "outputs": [],
      "source": [
        "for image, label in train.take(5):\n",
        "  plt.figure()\n",
        "  plt.imshow(image)\n",
        "  plt.title(get_label_name(label))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZlO9G4zFU-b"
      },
      "source": [
        "Now thi size of every image is same"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plcE4eQ8Fmo7"
      },
      "outputs": [],
      "source": [
        "#To check the image sizes\n",
        "for img, label in raw_train.take(5):\n",
        "  print(\"Original shape:\", img.shape)\n",
        "\n",
        "for img, label in train.take(5):\n",
        "  print(\"New shape:\", img.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20W1HZ_WFZyX"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "SHUFFLE_BUFFER_SIZE = 1000\n",
        "\n",
        "train_batches = train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "validation_batches = validation.batch(BATCH_SIZE)\n",
        "test_batches = test.batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd-48p9PbkJQ"
      },
      "source": [
        "##Picking a Pretrained Model\n",
        "Pretrained Model=Mobilenet V2 by google\n",
        "This have over a million pictures on 1000 classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OW0x0F00b-LC"
      },
      "outputs": [],
      "source": [
        "IMG_SHAPE=(160,160,3)\n",
        "base_model=tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,include_top=False,\n",
        "                                             weights='imagenet')\n",
        "base_model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dwe0_MA-dFP_"
      },
      "outputs": [],
      "source": [
        "#Now we have to freeze the training  meaning the weights won't be changing during the training\n",
        "base_model.trainable=False\n",
        "base_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lc9sjWZ0alij"
      },
      "source": [
        "###Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63zbH-cReT3a"
      },
      "outputs": [],
      "source": [
        "#Adding Classifier= GlobalAveragePooling\n",
        "global_avg=tf.keras.layers.GlobalAveragePooling2D()\n",
        "\n",
        "#Ass theres only 2 class to predict one either cat or dog\n",
        "#We  will add the output Dense layer in 1\n",
        "prediction=tf.keras.layers.Dense(1)\n",
        "\n",
        "#Creating Model\n",
        "model=tf.keras.Sequential([base_model,\n",
        "                           global_avg,\n",
        "                           prediction])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBMCz5nHau5m"
      },
      "source": [
        "###Compile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPXEFhsv-sSg"
      },
      "outputs": [],
      "source": [
        "#compiling\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "              loss='BinaryCrossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5tCzeA2axYl"
      },
      "source": [
        "###Fitting/training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5_T4u7HAGeJ"
      },
      "outputs": [],
      "source": [
        "#Fit/Train\n",
        "history=model.fit(train_batches,#whats this batches , and where is y train?\n",
        "          epochs=3,\n",
        "          validation_data=validation_batches,\n",
        "          )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Udg8G3Z0a1Eh"
      },
      "source": [
        "###Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70GHbGFWBmM6"
      },
      "outputs": [],
      "source": [
        "#Evaluate\n",
        "loss,accuracy=model.evaluate(test_batches,verbose=2)\n",
        "print(f'{accuracy:.03f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb8V_lM5a31C"
      },
      "source": [
        "###Saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YV60PY0ICxBS"
      },
      "outputs": [],
      "source": [
        "#Saving\n",
        "model.save('dogs_vs_cats.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KH10UO2a5vQ"
      },
      "source": [
        "###Loading the save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j27_WtDmDCwQ"
      },
      "outputs": [],
      "source": [
        "##To load the saved model\n",
        "new_model=tf.keras.models.load_model('dogs_vs_cats.h5')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMSEi6KRGcNJ64hFJMlPv0y",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}